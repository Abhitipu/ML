Read data

Using cosine_similarity
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9719806763285024 for 5 neighbors

              precision    recall  f1-score   support

         Ham       0.71      1.00      0.83       724
        Spam       1.00      0.06      0.12       311

    accuracy                           0.72      1035
   macro avg       0.86      0.53      0.47      1035
weighted avg       0.80      0.72      0.62      1035


 Accuracy v/s number of neighbors table

0.957487922705314 		 1 
0.9449275362318841 		 101 
0.9246376811594202 		 201 
0.8985507246376812 		 301 
0.8840579710144928 		 401 
0.8666666666666667 		 501 
0.8502415458937198 		 601 
0.8376811594202899 		 701 
0.8183574879227054 		 801 
0.7990338164251207 		 901 
0.782608695652174 		 1001 
0.7758454106280194 		 1101 
0.7652173913043478 		 1201 
0.7574879227053141 		 1301 
0.7439613526570048 		 1401 
0.7352657004830918 		 1501 
0.7256038647342995 		 1601 
0.7217391304347827 		 1701 
0.7207729468599033 		 1801 
0.7207729468599033 		 1901 
0.7198067632850241 		 2001 
0.7198067632850241 		 2101 
0.7198067632850241 		 2201 
0.7188405797101449 		 2301 
0.7188405797101449 		 2401 
0.7178743961352657 		 2501 
0.7178743961352657 		 2601 
0.7178743961352657 		 2701 
0.7178743961352657 		 2801 
0.7178743961352657 		 2901 
0.7178743961352657 		 3001 
0.7178743961352657 		 3101 
0.7178743961352657 		 3201 
0.7178743961352657 		 3301 
0.7178743961352657 		 3401 
0.7178743961352657 		 3501 
0.7178743961352657 		 3601 
0.7178743961352657 		 3701 
0.7178743961352657 		 3801 
0.7178743961352657 		 3901 
0.7178743961352657 		 4001 
0.7178743961352657 		 4101 

Using manhattan_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.8917874396135266 for 202 neighbors

              precision    recall  f1-score   support

         Ham       0.71      1.00      0.83       724
        Spam       1.00      0.04      0.07       311

    accuracy                           0.71      1035
   macro avg       0.85      0.52      0.45      1035
weighted avg       0.80      0.71      0.60      1035


 Accuracy v/s number of neighbors table

0.7652173913043478 		 1 
0.5826086956521739 		 101 
0.8908212560386474 		 201 
0.7342995169082126 		 301 
0.7188405797101449 		 401 
0.7169082125603865 		 501 
0.714975845410628 		 601 
0.7140096618357488 		 701 
0.7140096618357488 		 801 
0.7120772946859903 		 901 
0.7111111111111111 		 1001 
0.7111111111111111 		 1101 
0.7111111111111111 		 1201 
0.7111111111111111 		 1301 
0.7111111111111111 		 1401 
0.7111111111111111 		 1501 
0.7111111111111111 		 1601 
0.7111111111111111 		 1701 
0.7111111111111111 		 1801 
0.7111111111111111 		 1901 
0.7111111111111111 		 2001 
0.7111111111111111 		 2101 
0.7111111111111111 		 2201 
0.7111111111111111 		 2301 
0.7111111111111111 		 2401 
0.7111111111111111 		 2501 
0.7111111111111111 		 2601 
0.7111111111111111 		 2701 
0.7111111111111111 		 2801 
0.7111111111111111 		 2901 
0.7111111111111111 		 3001 
0.7111111111111111 		 3101 
0.7101449275362319 		 3201 
0.7101449275362319 		 3301 
0.7101449275362319 		 3401 
0.7101449275362319 		 3501 
0.7101449275362319 		 3601 
0.7101449275362319 		 3701 
0.7101449275362319 		 3801 
0.7101449275362319 		 3901 
0.7101449275362319 		 4001 
0.7101449275362319 		 4101 

Using euclidian_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9710144927536232 for 5 neighbors

              precision    recall  f1-score   support

         Ham       0.70      1.00      0.83       724
        Spam       1.00      0.03      0.05       311

    accuracy                           0.71      1035
   macro avg       0.85      0.51      0.44      1035
weighted avg       0.79      0.71      0.59      1035


 Accuracy v/s number of neighbors table

0.957487922705314 		 1 
0.9429951690821256 		 101 
0.9082125603864735 		 201 
0.8859903381642512 		 301 
0.8734299516908213 		 401 
0.8570048309178744 		 501 
0.8396135265700483 		 601 
0.8270531400966183 		 701 
0.8067632850241546 		 801 
0.7903381642512077 		 901 
0.7758454106280194 		 1001 
0.7681159420289855 		 1101 
0.7507246376811594 		 1201 
0.740096618357488 		 1301 
0.7275362318840579 		 1401 
0.7178743961352657 		 1501 
0.7101449275362319 		 1601 
0.7082125603864734 		 1701 
0.7072463768115942 		 1801 
0.7072463768115942 		 1901 
0.7072463768115942 		 2001 
0.7072463768115942 		 2101 
0.7072463768115942 		 2201 
0.7072463768115942 		 2301 
0.7072463768115942 		 2401 
0.7072463768115942 		 2501 
0.7072463768115942 		 2601 
0.7072463768115942 		 2701 
0.7072463768115942 		 2801 
0.7072463768115942 		 2901 
0.7072463768115942 		 3001 
0.7072463768115942 		 3101 
0.7072463768115942 		 3201 
0.7072463768115942 		 3301 
0.7072463768115942 		 3401 
0.7072463768115942 		 3501 
0.7072463768115942 		 3601 
0.7072463768115942 		 3701 
0.7072463768115942 		 3801 
0.7072463768115942 		 3901 
0.7072463768115942 		 4001 
0.7072463768115942 		 4101 
Done
