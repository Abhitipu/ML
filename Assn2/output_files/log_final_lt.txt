Read data

Using cosine_similarity
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9748792270531401 for 11 neighbors

              precision    recall  f1-score   support

         Ham       0.97      0.99      0.98       740
        Spam       0.98      0.93      0.95       295

    accuracy                           0.97      1035
   macro avg       0.98      0.96      0.97      1035
weighted avg       0.97      0.97      0.97      1035


 Accuracy v/s number of neighbors table

1 		 0.966183574879227 
101 		 0.9594202898550724 
201 		 0.9314009661835749 
301 		 0.9091787439613527 
401 		 0.8898550724637682 
501 		 0.8714975845410629 
601 		 0.855072463768116 
701 		 0.8444444444444444 
801 		 0.8299516908212561 
901 		 0.8202898550724638 
1001 		 0.8048309178743961 
1101 		 0.7990338164251207 
1201 		 0.7913043478260869 
1301 		 0.7777777777777778 
1401 		 0.7681159420289855 
1501 		 0.7545893719806763 
1601 		 0.748792270531401 
1701 		 0.744927536231884 
1801 		 0.7429951690821256 
1901 		 0.7381642512077294 
2001 		 0.736231884057971 
2101 		 0.736231884057971 
2201 		 0.7352657004830918 
2301 		 0.7342995169082126 
2401 		 0.7342995169082126 
2501 		 0.7342995169082126 
2601 		 0.7342995169082126 
2701 		 0.7342995169082126 
2801 		 0.7342995169082126 
2901 		 0.7342995169082126 
3001 		 0.7342995169082126 
3101 		 0.7342995169082126 
3201 		 0.7342995169082126 
3301 		 0.7342995169082126 
3401 		 0.7342995169082126 
3501 		 0.7342995169082126 
3601 		 0.7342995169082126 
3701 		 0.7342995169082126 
3801 		 0.7342995169082126 
3901 		 0.7342995169082126 
4001 		 0.7342995169082126 
4101 		 0.7342995169082126 

Using manhattan_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.914975845410628 for 210 neighbors

              precision    recall  f1-score   support

         Ham       0.97      0.90      0.93       740
        Spam       0.79      0.92      0.85       295

    accuracy                           0.91      1035
   macro avg       0.88      0.91      0.89      1035
weighted avg       0.92      0.91      0.91      1035


 Accuracy v/s number of neighbors table

1 		 0.7478260869565218 
101 		 0.5971014492753624 
201 		 0.8879227053140096 
301 		 0.7497584541062802 
401 		 0.736231884057971 
501 		 0.7333333333333333 
601 		 0.7323671497584541 
701 		 0.7314009661835749 
801 		 0.7314009661835749 
901 		 0.7314009661835749 
1001 		 0.7314009661835749 
1101 		 0.7304347826086957 
1201 		 0.7304347826086957 
1301 		 0.7304347826086957 
1401 		 0.7304347826086957 
1501 		 0.7304347826086957 
1601 		 0.7304347826086957 
1701 		 0.7304347826086957 
1801 		 0.7304347826086957 
1901 		 0.7304347826086957 
2001 		 0.7304347826086957 
2101 		 0.7304347826086957 
2201 		 0.7304347826086957 
2301 		 0.7304347826086957 
2401 		 0.7304347826086957 
2501 		 0.7304347826086957 
2601 		 0.7304347826086957 
2701 		 0.7304347826086957 
2801 		 0.7304347826086957 
2901 		 0.7304347826086957 
3001 		 0.7294685990338164 
3101 		 0.7294685990338164 
3201 		 0.7294685990338164 
3301 		 0.7294685990338164 
3401 		 0.7294685990338164 
3501 		 0.7294685990338164 
3601 		 0.7294685990338164 
3701 		 0.7294685990338164 
3801 		 0.7294685990338164 
3901 		 0.7294685990338164 
4001 		 0.7294685990338164 
4101 		 0.7294685990338164 

Using euclidian_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9739130434782609 for 11 neighbors

              precision    recall  f1-score   support

         Ham       0.97      0.99      0.98       740
        Spam       0.98      0.93      0.95       295

    accuracy                           0.97      1035
   macro avg       0.98      0.96      0.97      1035
weighted avg       0.97      0.97      0.97      1035


 Accuracy v/s number of neighbors table

1 		 0.966183574879227 
101 		 0.9545893719806763 
201 		 0.923671497584541 
301 		 0.9004830917874396 
401 		 0.8782608695652174 
501 		 0.8599033816425121 
601 		 0.8473429951690822 
701 		 0.8357487922705314 
801 		 0.8173913043478261 
901 		 0.8077294685990338 
1001 		 0.7922705314009661 
1101 		 0.7864734299516908 
1201 		 0.77487922705314 
1301 		 0.7632850241545893 
1401 		 0.7507246376811594 
1501 		 0.7429951690821256 
1601 		 0.7352657004830918 
1701 		 0.7304347826086957 
1801 		 0.7265700483091787 
1901 		 0.7265700483091787 
2001 		 0.7265700483091787 
2101 		 0.7265700483091787 
2201 		 0.7265700483091787 
2301 		 0.7265700483091787 
2401 		 0.7265700483091787 
2501 		 0.7265700483091787 
2601 		 0.7265700483091787 
2701 		 0.7265700483091787 
2801 		 0.7265700483091787 
2901 		 0.7265700483091787 
3001 		 0.7265700483091787 
3101 		 0.7265700483091787 
3201 		 0.7265700483091787 
3301 		 0.7265700483091787 
3401 		 0.7265700483091787 
3501 		 0.7265700483091787 
3601 		 0.7265700483091787 
3701 		 0.7265700483091787 
3801 		 0.7265700483091787 
3901 		 0.7265700483091787 
4001 		 0.7265700483091787 
4101 		 0.7265700483091787 
Done
