Read data

Using cosine_similarity
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9642512077294686 for 55 neighbors

              precision    recall  f1-score   support

         Ham       0.95      0.99      0.97       735
        Spam       0.98      0.88      0.93       300

    accuracy                           0.96      1035
   macro avg       0.97      0.94      0.95      1035
weighted avg       0.96      0.96      0.96      1035


 Accuracy v/s number of neighbors table

1 		 0.957487922705314 
101 		 0.9565217391304348 
201 		 0.9352657004830918 
301 		 0.9120772946859903 
401 		 0.8879227053140096 
501 		 0.8724637681159421 
601 		 0.8570048309178744 
701 		 0.8386473429951691 
801 		 0.8280193236714976 
901 		 0.8135265700483092 
1001 		 0.7980676328502415 
1101 		 0.7835748792270532 
1201 		 0.77487922705314 
1301 		 0.7613526570048309 
1401 		 0.7536231884057971 
1501 		 0.7420289855072464 
1601 		 0.7381642512077294 
1701 		 0.7294685990338164 
1801 		 0.7275362318840579 
1901 		 0.7275362318840579 
2001 		 0.7275362318840579 
2101 		 0.7265700483091787 
2201 		 0.7265700483091787 
2301 		 0.7265700483091787 
2401 		 0.7265700483091787 
2501 		 0.7265700483091787 
2601 		 0.7265700483091787 
2701 		 0.7265700483091787 
2801 		 0.7256038647342995 
2901 		 0.7256038647342995 
3001 		 0.7246376811594203 
3101 		 0.7246376811594203 
3201 		 0.7246376811594203 
3301 		 0.7246376811594203 
3401 		 0.7236714975845411 
3501 		 0.7236714975845411 
3601 		 0.7236714975845411 
3701 		 0.7236714975845411 
3801 		 0.7236714975845411 
3901 		 0.7236714975845411 
4001 		 0.7236714975845411 
4101 		 0.7236714975845411 

Using manhattan_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.8801932367149758 for 219 neighbors

              precision    recall  f1-score   support

         Ham       0.95      0.87      0.91       735
        Spam       0.74      0.88      0.80       300

    accuracy                           0.87      1035
   macro avg       0.84      0.88      0.86      1035
weighted avg       0.89      0.87      0.88      1035


 Accuracy v/s number of neighbors table

1 		 0.740096618357488 
101 		 0.5623188405797102 
201 		 0.8154589371980676 
301 		 0.740096618357488 
401 		 0.7275362318840579 
501 		 0.7246376811594203 
601 		 0.7246376811594203 
701 		 0.7207729468599033 
801 		 0.7207729468599033 
901 		 0.7207729468599033 
1001 		 0.7207729468599033 
1101 		 0.7207729468599033 
1201 		 0.7207729468599033 
1301 		 0.7207729468599033 
1401 		 0.7207729468599033 
1501 		 0.7207729468599033 
1601 		 0.7207729468599033 
1701 		 0.7207729468599033 
1801 		 0.7207729468599033 
1901 		 0.7207729468599033 
2001 		 0.7207729468599033 
2101 		 0.7207729468599033 
2201 		 0.7207729468599033 
2301 		 0.7207729468599033 
2401 		 0.7207729468599033 
2501 		 0.7207729468599033 
2601 		 0.7207729468599033 
2701 		 0.7207729468599033 
2801 		 0.7207729468599033 
2901 		 0.7207729468599033 
3001 		 0.7207729468599033 
3101 		 0.7207729468599033 
3201 		 0.7207729468599033 
3301 		 0.7207729468599033 
3401 		 0.7207729468599033 
3501 		 0.7207729468599033 
3601 		 0.7207729468599033 
3701 		 0.7207729468599033 
3801 		 0.7207729468599033 
3901 		 0.7207729468599033 
4001 		 0.7207729468599033 
4101 		 0.7207729468599033 

Using euclidian_distance
Training set dimensions: (4136, 50140)
Test set dimensions: (1035, 50140)
Obtained best accuracy 0.9632850241545894 for 55 neighbors

              precision    recall  f1-score   support

         Ham       0.95      0.99      0.97       735
        Spam       0.98      0.88      0.93       300

    accuracy                           0.96      1035
   macro avg       0.97      0.94      0.95      1035
weighted avg       0.96      0.96      0.96      1035


 Accuracy v/s number of neighbors table

1 		 0.957487922705314 
101 		 0.9507246376811594 
201 		 0.9256038647342996 
301 		 0.9004830917874396 
401 		 0.8763285024154589 
501 		 0.8579710144927536 
601 		 0.8444444444444444 
701 		 0.8280193236714976 
801 		 0.8144927536231884 
901 		 0.7942028985507247 
1001 		 0.7884057971014493 
1101 		 0.7690821256038647 
1201 		 0.7642512077294686 
1301 		 0.7507246376811594 
1401 		 0.7420289855072464 
1501 		 0.7333333333333333 
1601 		 0.7275362318840579 
1701 		 0.7217391304347827 
1801 		 0.7198067632850241 
1901 		 0.7198067632850241 
2001 		 0.7198067632850241 
2101 		 0.7198067632850241 
2201 		 0.7198067632850241 
2301 		 0.7198067632850241 
2401 		 0.7198067632850241 
2501 		 0.7198067632850241 
2601 		 0.7198067632850241 
2701 		 0.7198067632850241 
2801 		 0.7198067632850241 
2901 		 0.7198067632850241 
3001 		 0.7198067632850241 
3101 		 0.7198067632850241 
3201 		 0.7198067632850241 
3301 		 0.7198067632850241 
3401 		 0.7198067632850241 
3501 		 0.7198067632850241 
3601 		 0.7198067632850241 
3701 		 0.7198067632850241 
3801 		 0.7198067632850241 
3901 		 0.7198067632850241 
4001 		 0.7198067632850241 
4101 		 0.7198067632850241 
Done
